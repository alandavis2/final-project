{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Anatomy Chatbot with BERT\n",
    "\n",
    "This notebook implements a chatbot for human anatomy using BERT and Flask. Follow the cells in order to:\n",
    "1. Set up the environment\n",
    "2. Load and prepare data\n",
    "3. Train the model\n",
    "4. Launch the API server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from flask import Flask, request, jsonify\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized successfully\n"
     ]
    }
   ],
   "source": [
    "class AnatomyChatbot:\n",
    "    def __init__(self, model_name: str = \"bert-base-uncased\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name, \n",
    "            num_labels=2\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def load_and_prepare_dataset(self, folder_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Load and prepare dataset from JSONL files\"\"\"\n",
    "        all_data = []\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise FileNotFoundError(f\"Directory {folder_path} not found\")\n",
    "            \n",
    "        jsonl_files = [f for f in os.listdir(folder_path) if f.endswith('.jsonl')]\n",
    "        if not jsonl_files:\n",
    "            raise FileNotFoundError(f\"No JSONL files found in {folder_path}\")\n",
    "            \n",
    "        for file_name in jsonl_files:\n",
    "            with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                all_data.extend([json.loads(line) for line in file])\n",
    "        \n",
    "        df = pd.DataFrame(all_data)\n",
    "        if 'title' not in df.columns or 'content' not in df.columns:\n",
    "            raise ValueError(\"Dataset missing required columns: title and content\")\n",
    "            \n",
    "        # Combine title and content into a single text field\n",
    "        df['text'] = df['title'] + \" \" + df['content']\n",
    "        \n",
    "        # Add dummy labels for binary classification (you should replace this with your actual labels)\n",
    "        df['labels'] = 0  # Replace with actual labels if you have them\n",
    "        \n",
    "        return df[['text', 'labels']]  # Only keep the columns we need\n",
    "    \n",
    "    def split_dataset(self, df: pd.DataFrame, train_ratio: float = 0.8) -> tuple:\n",
    "        \"\"\"Split dataset into training and testing sets\"\"\"\n",
    "        train_size = int(len(df) * train_ratio)\n",
    "        train_data = df.iloc[:train_size]\n",
    "        test_data = df.iloc[train_size:]\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def tokenize_function(self, examples: Dict[str, Any]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize the input text\"\"\"\n",
    "        return self.tokenizer(\n",
    "            examples['text'],  # Changed from 'contents' to 'text'\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    def compute_metrics(self, pred) -> Dict[str, Any]:\n",
    "        \"\"\"Compute evaluation metrics\"\"\"\n",
    "        labels = pred.label_ids\n",
    "        preds = np.argmax(pred.predictions, axis=1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"confusion_matrix\": confusion_matrix(labels, preds).tolist()\n",
    "        }\n",
    "    \n",
    "    def train(self, train_dataset, test_dataset, output_dir: str = \"./results\"):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "            logging_steps=10,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            greater_is_better=True,\n",
    "            remove_unused_columns=False\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=test_dataset,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        return trainer.evaluate()\n",
    "    \n",
    "    def predict(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Make prediction for input text\"\"\"\n",
    "        self.model.eval()\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            outputs = self.model(**inputs)\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "        prediction = outputs.logits.argmax().item()\n",
    "        confidence = torch.softmax(outputs.logits, dim=1).max().item()\n",
    "        \n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"confidence\": confidence,\n",
    "            \"response_time\": response_time\n",
    "        }\n",
    "\n",
    "# Initialize the chatbot\n",
    "chatbot = AnatomyChatbot()\n",
    "print(\"Chatbot initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Shape: (12060, 2)\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anatomy_Gray What is anatomy? Anatomy includes...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anatomy_Gray Observation and visualization are...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anatomy_Gray How can gross anatomy be studied?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anatomy_Gray This includes the vasculature, th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anatomy_Gray Each of these approaches has bene...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  Anatomy_Gray What is anatomy? Anatomy includes...       0\n",
       "1  Anatomy_Gray Observation and visualization are...       0\n",
       "2  Anatomy_Gray How can gross anatomy be studied?...       0\n",
       "3  Anatomy_Gray This includes the vasculature, th...       0\n",
       "4  Anatomy_Gray Each of these approaches has bene...       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "# Update this path to where your JSONL files are stored\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "try:\n",
    "    # Load dataset\n",
    "    dataset = chatbot.load_and_prepare_dataset(DATA_DIR)\n",
    "    print(f\"Dataset loaded successfully. Shape: {dataset.shape}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample data:\")\n",
    "    display(dataset.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8345308027f49c9a28abdadebc42108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9648 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4d0f6355af4caf81f638902c9ce951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2412 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 9648\n",
      "Testing set size: 2412\n"
     ]
    }
   ],
   "source": [
    "# Split and prepare datasets\n",
    "train_data, test_data = chatbot.split_dataset(dataset)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Map the tokenization function across the datasets\n",
    "train_dataset = train_dataset.map(chatbot.tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(chatbot.tokenize_function, batched=True)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3618' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/3618 00:55 < 55:50:46, 0.02 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model\n",
    "try:\n",
    "    print(\"Starting model training...\")\n",
    "    metrics = chatbot.train(train_dataset, test_dataset)\n",
    "    print(\"\\nTraining completed. Final metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_endpoint():\n",
    "    try:\n",
    "        data = request.json\n",
    "        if not data or 'input' not in data:\n",
    "            return jsonify({\"error\": \"No input provided\"}), 400\n",
    "            \n",
    "        result = chatbot.predict(data['input'])\n",
    "        return jsonify(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Start the Flask server\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "Once the Flask server is running, you can test it using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Test the API\n",
    "test_text = \"The human heart has four chambers\"\n",
    "response = requests.post('http://localhost:5000/predict', \n",
    "                        json={'input': test_text})\n",
    "\n",
    "print(\"API Response:\")\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
